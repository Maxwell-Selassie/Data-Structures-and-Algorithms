{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words :  881\n",
      "Most frequent word :  [('and', 56)]\n",
      "Unique words :  {'of', 'recall', 'manipulation', 'unsupervised', 'appropriate', 'building', 'route', 'policy', 'to\\ncomplex', 'heavily', 'model\\npredictions', 'make', 'clean', 'inform\\ntrading', 'monitoring', 'supervised', 'relationship', 'methods\\nhelp', 'randomized', 'numerical', 'especially', 'with', 'models\\nin', 'discrete', 'practicesâ€”such', 'NumPy', 'research', 'error', 'attempt', 'In', 'them', 'Calculus', 'remarkable', 'commonly', 'tools', 'reduce', 'justice', 'GPU', 'calculus\\nhelps', 'labels', '--', 'performance\\ndegrades', 'summarization', 'conferences', 'classical', 'concepts', 'capabilities', 'driving\\n\\nAs', 'practice', 'because', 'F1-score', 'generalization', 'interpretability', 'much', 'scaling', 'footprint)', 'growing', 'maintainable', 'tasks', 'provide', 'Feature', 'for', 'input', 'reinforcement', 'anomaly', 'gradients', 'transformer-based', 'theory', 'other', 'TensorFlow', 'useful', 'for\\nclassification', 'try', 'platforms', 'adapts', '(including', 'detection', 'ranging', 'computation', 'engineers', 'statistical', 'domain', 'journals', 'exploratory', 'increasingly', 'mitigate', 'code', 'same', 'computing', 'DevOps', 'find', 'sample', 'reliably', 'paradigm', 'Understanding', 'networks', 'often', 'serves', 'gives', 'Natural', 'outputs', 'describe', 'perform', 'playing', 'continuous', 'computers', 'leverages', 'MLOps', 'expanding', 'practicing', 'analyze', 'and\\ndomain', 'remain', 'finally', 'simple', 'processing', 'autonomous', 'and\\ncarbon', 'environment', 'on', 'facial\\nrecognition', 'discipline', 'deployment', 'multivariable', 'long-form', 'where', 'evaluated', 'updating', 'current', 'open-source', 'careful', 'modern', 'creating', 'evolves', 'researchers', 'healthcare', 'the\\nlanguage', 'high-stakes\\ndomains', 'production', 'domains', 'improve', 'These\\nframeworks', 'as', 'crucial', 'gradient', 'reproducibility', 'systems\\nAt', 'steps', 'have', 'projects\\n\\nThis', 'study', 'hyperparameter', 'diversity\\nor', 'field', 'curve', 'tuning', 'learners', 'and', 'distribution', 'available', 'Retail', 'also', 'technical', 'search', 'such', 'machine', 'principles', 'SHAP', 'classification', 'performance\\nand', 'boundaries', 'filter', 'fitting', 'knowledge', 'while', 'regulatory', 'experiment', 'learn', 'deals', 'polynomial', 'translation', 'hypothesis', 'pipelines\\n\\nApplications', 'lemmatization', 'utilities', 'vast', 'maximize', 'underpin', 'form', 'wrapper', 'that', 'acceleration', 'new', 'descent', 'For', 'the', 'squared', 'Examples', 'computer', 'variables', 'requires', 'regression', 'energy', 'values', 'example', 'results', 'deep', 'particularly', 'structures\\nor', 'over', 'science', 'sentiment', 'categorical', 'biases', 'includes', 'features', 'an', 'about\\nuncertainty', 'tasks\\nwhich', 'used', 'software', 'optimization', 'deployed', 'discover', 'mathematical', 'sentence', 'Bayesian', 'known', 'understanding', 'respect', 'attention', 'in', 'trust', 'interacting', 'clustering', 'learning\\npractical', 'critical', 'grid', 'area', 'evaluate', 'possible', 'engagement', 'recommendation', 'changes', 'hand', 'LIME', 'considerations', 'version', 'frequencies', 'third', 'metrics:', 'principal', 'Libraries', 'automatic', 'systems\\n\\nAt', 'iterative', 'loss\\nfunctions', 'imaging\\npatient', 'include', 'a', 'many', 'algebra', 'powers\\nchatbots', '\\ncomputer', 'Clustering', 'explicitly', 'model', 'feedforward', 'Each', 'high-level', 'dimensionality', 'achieve', 'concerned', 'reward', 'drift', 'without', 'stopword', 'how', 'inference\\n\\nModern', 'analysis\\nto', 'decision', 'lexical', 'modular', 'dataset', 'personalize', 'PyTorch', 'medicine', 'enables', 'variety', 'game', 'has\\nproduced', 'ethical', 'stemming', 'time\\n\\nThe', 'statistics', 'their', 'community', 'data', 'Q-learning\\nand', 'construct', 'decisions', 'logistic', 'successful', 'monitor', 'learning\\nis', 'are', 'inputs', 'testing', 'fundamental', 'credit', 'Reinforcement', 'sustainability', 'Algorithms', 'practitioners', 'natural', 'reduction', 'accuracy', 'interdisciplinary', 'one\\nof', 'standard', 'encoding', 'helps', 'Advances', 'interaction', 'automation', 'fraud', 'DBSCAN\\nare', 'ability', 'help', 'which', 'design', 'hardware', 'fairness', 'trained', '(PCA)', 'component', 'while\\ntransportation', 'Here', 'consideration', 'trees', 'analysis', 'popular', 'most', 'produce', 'concept', 'algorithm', 'allow', 'lengths', 'provides', 'is', 'Machine', 'dataset\\nA', 'deploy', 'Through', 'reduction\\n\\nThe', 'reference', 'self-supervised', 'essential;', 'by', 'word', 'The', 'acquire', 'Monitoring', 'engineer', 'methods\\n\\nReinforcement', '(MSE)', 'frameworks', 'help\\nreduce', 'patterns', 'Cross-validation', 'step', 'reasoning', 'important', 'architectures', 'apply', 'have\\nbecome', 'gaining', 'error\\nthey', 'whereas', 'workflow:', 'removal', 'output', 'labeled', 'matrices', 'efficiency', 'differentiation', 'adjust', 'programmed\\nAt', 'forecasting', 'essential', 'engineering', 'handling\\nmissing', 'frequency', 'categories', 'scikit-learn', 'tokenization', 'Good', 'criminal', 'continues', 'relies', 'methods', 'detect', 'from', 'control', 'its', 'estimate', 'explore', 'time', 'hierarchical', 'rewards', 'metrics\\nsuch', 'sectors', 'processing\\n\\nFeature', 'it', 'diagnostic', 'using', 'Dimensionality', 'file', 'staying', 'simplest', 'finance', 'model\\nsettings\\n\\nModel', 'trial', 'learning\\nfocuses', 'problems', 'embedded', 'fairness-aware', 'build', 'preserving', 'selection', 'best', 'user', 'under', 'predict', 'Supervised', 'be', 'can', 'stratification', 'unlabeled', 'blogs', 'ROC', 'neural', 'key', \"model's\", 'being', 'or', 'what', 'You', 'vision', 'object', 'metrics', 'number', 'learning', 'penalties', 'features\\nselect', 'preprocessing', 'foundations', 'demand', 'precision', 'correct', 'compliance', 'push', 'Unsupervised', 'hidden', 'Python', 'core', 'Tools', 'robotics', 'vectors', 'situations', 'and\\npandas', 'risk', 'mean', 'terms\\nor', 'user-friendly\\ninterface', 'cumulative', 'models', 'This', 'techniques', 'variance', 'experiences', 'become', 'specialized', 'personalized', 'behaviour', 'algorithms', 'use', 'language', 'problems\\ncommon', 'systems', 'used\\n\\nUnsupervised', 'becomes', 'explanations', 'CICDâ€”\\nare', 'absolute', 'providing', 'other\\ntext-based', 'like', 'strategies', 'level', 'model\\narchitectures', 'drawing', 'between', 'text', 'to', 'training', 'in\\nsociety', 'train', '(MAE)', 'e-commerce', 'Probability', 'agents', 'normalizing', 'involves', 'k-means', 'regression\\nclustering', 'Linear', 'deeper', 'parameters', 'experimentation', 'receive', 'through'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def text_analyzer(filename):\n",
    "    try:\n",
    "        with open(filename,'r') as file:\n",
    "            words = file.read()\n",
    "            if words:\n",
    "                cleaned_text = re.sub(r'[^\\w\\s]+','',words.strip())\n",
    "        return cleaned_text\n",
    "    except FileNotFoundError:\n",
    "        return ''\n",
    "    \n",
    "\n",
    "def count_words(cleaned_text):\n",
    "    try:\n",
    "        return sum(1 for word in cleaned_text.split())\n",
    "    except (FileNotFoundError,ValueError):\n",
    "        print('File not found!')\n",
    "\n",
    "\n",
    "def most_frequent(cleaned_text):\n",
    "    try:\n",
    "        word_count = Counter(cleaned_text.lower().split())\n",
    "        return word_count.most_common(1)\n",
    "    except FileNotFoundError:\n",
    "        print('File not found')\n",
    "\n",
    "\n",
    "def unique_words(cleaned_text):\n",
    "    try:\n",
    "        return set(cleaned_text.split())\n",
    "    except Exception as e:\n",
    "        print('Error : ',e)\n",
    "\n",
    "\n",
    "def main():\n",
    "    filename = 'ml.txt'\n",
    "    cleaned_text = text_analyzer(filename)\n",
    "\n",
    "    if cleaned_text:\n",
    "        print('Number of words : ',count_words(cleaned_text))\n",
    "        print('Most frequent word : ',most_frequent(cleaned_text))\n",
    "        print('Unique words : ',unique_words(cleaned_text))\n",
    "    else:\n",
    "        print('No text to analyze')\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
